{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c536a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pathlib\n",
    "import tempfile\n",
    "import threading\n",
    "import time\n",
    "import zipfile\n",
    "from datetime import datetime\n",
    "from typing import Optional, List, Set, Dict\n",
    "\n",
    "import requests\n",
    "import sqlite3\n",
    "from bs4 import BeautifulSoup\n",
    "from requests import Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1cc1a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTICLES_REQ_URL = \"https://www.binance.com/bapi/composite/v1/public/cms/news/queryFlashNewsList\"\n",
    "CONCRETE_ARTICLE_URL = \"https://www.binance.com/en/news/flash/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ef16dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SqlliteConnector:\n",
    "    conn: sqlite3.Connection\n",
    "    cursor: sqlite3.Cursor\n",
    "\n",
    "    def __init__(self):\n",
    "        self.conn = sqlite3.connect(\"./articles.db\")\n",
    "        self.cursor = self.conn.cursor()\n",
    "        self.conn.execute('''\n",
    "        create table if not exists articles (id INTEGER PRIMARY KEY)\n",
    "        ''')\n",
    "\n",
    "    def insert_article(self, article: int):\n",
    "        self.cursor.execute(\"insert into articles (id) values (?)\", (article,))\n",
    "        self.conn.commit()\n",
    "\n",
    "    def is_parsed(self, article: int):\n",
    "        self.conn.execute(\"select * from articles where id = ?\", (article,))\n",
    "        row = self.cursor.fetchone()\n",
    "        return row is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0aacbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RequestInspector:\n",
    "    lock: threading.Lock\n",
    "    hour_limit: int\n",
    "    minute_limit: int\n",
    "    delta: int\n",
    "    last_parsed_time: float\n",
    "    headers: dict\n",
    "\n",
    "    def __init__(self, hour_limit=720000, headers: dict = None):\n",
    "        if headers is None:\n",
    "            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; rv:91.0) Gecko/20100101 Firefox/91.0'}\n",
    "        self.hour_limit = hour_limit\n",
    "        self.last_parsed_time = time.time()\n",
    "        self.lock = threading.Lock()\n",
    "        self.minute_limit = int(hour_limit / 60)\n",
    "        self.delta = 0\n",
    "        self.headers = headers\n",
    "\n",
    "    def req(self, url: str, params: Dict) -> Response:\n",
    "        with self.lock:\n",
    "            self.delta += 1\n",
    "            now = time.time()\n",
    "            time_delta = 60 - (now - self.last_parsed_time)\n",
    "            if time_delta < 60:\n",
    "                if self.delta >= self.minute_limit:\n",
    "                    time.sleep(time_delta)\n",
    "                    self.last_parsed_time = now + time_delta\n",
    "                    self.delta = 0\n",
    "            else:\n",
    "                self.last_parsed_time = now\n",
    "                self.delta = 0\n",
    "            return requests.get(url, params=params, headers=self.headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8738b0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RequestIterator:\n",
    "    index: int\n",
    "    articles: List\n",
    "    prev_size: int\n",
    "    page: int\n",
    "    inspector: RequestInspector\n",
    "\n",
    "    def __init__(self, inspector: RequestInspector):\n",
    "        self.index = 0\n",
    "        self.inspector = inspector\n",
    "        self.page = 1\n",
    "        self.prev_size = 20\n",
    "        self.articles = list()\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.index == len(self.articles) and self.prev_size == 20:\n",
    "            resp = self.inspector.req(ARTICLES_REQ_URL,\n",
    "                                      {\"pageNo\": self.page, \"pageSize\": 20, \"isTransform\": \"false\", \"tagId\": \"\"})\n",
    "            self.page += 1\n",
    "            if resp.status_code < 400:\n",
    "                new_articles_data = resp.json().get(\"data\").get(\"contents\")\n",
    "                self.articles.extend(new_articles_data)\n",
    "                self.prev_size = len(new_articles_data)\n",
    "        if self.index < len(self.articles):\n",
    "            value = self.articles[self.index]\n",
    "            self.index += 1\n",
    "            return value\n",
    "        else:\n",
    "            raise StopIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cca09e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RequestError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "class ContentParsingError(Exception):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33266725",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArticleInfo:\n",
    "    header: str\n",
    "    content: str\n",
    "    publication_dt: datetime\n",
    "    parsing_dt: datetime\n",
    "    html: str\n",
    "    href: str\n",
    "    meta_keywords: List[str]\n",
    "        \n",
    "class ArticleEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, datetime):\n",
    "            return obj.strftime('%Y-%m-%dT%H:%M:%S%z')\n",
    "        return super().default(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e32eda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_articles_binance(from_dt: Optional[datetime], to_dt: Optional[datetime], inspector: RequestInspector) -> \\\n",
    "        Set[int]:\n",
    "    if from_dt is None:\n",
    "        from_dt = datetime.now()\n",
    "    if to_dt is None:\n",
    "        to_dt = datetime(1970, 1, 1, 1)\n",
    "    articles = set()\n",
    "    from_: int = int(from_dt.timestamp() * 1000)\n",
    "    to_: int = int(to_dt.timestamp() * 1000)\n",
    "    for i in RequestIterator(inspector):\n",
    "        create_time: int = i.get(\"createTime\")\n",
    "        if create_time > from_:\n",
    "            continue\n",
    "        if create_time < to_:\n",
    "            break\n",
    "        articles.add(i.get('id'))\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31b14e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_info(href: str, inspector: RequestInspector) -> str:\n",
    "    res = inspector.req(href, {})\n",
    "    if res.status_code >= 400:\n",
    "        raise RequestError(f\"error while getting resource with status code - {res.status_code}\")\n",
    "    return res.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd448c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_disk(file_name: str, article: ArticleInfo) -> None:\n",
    "    path = pathlib.Path(f'./uploads/binance/{file_name}.zip')\n",
    "    with tempfile.NamedTemporaryFile(mode='w', suffix='.html', delete=True) as json_file, \\\n",
    "            tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=True) as html_file, \\\n",
    "            zipfile.ZipFile(path, 'w') as zipf:\n",
    "        html_file.write(article.html)\n",
    "        article_json = json.dumps(article.__dict__, cls=ArticleEncoder)\n",
    "        json_file.write(article_json)\n",
    "        zipf.write(json_file.name, f'{file_name}.json')\n",
    "        zipf.write(html_file.name, f'{file_name}.html')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78cce093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_articles(from_dt: Optional[datetime], to_dt: Optional[datetime]) -> None:\n",
    "    inspector: RequestInspector = RequestInspector()\n",
    "    conn = SqlliteConnector()\n",
    "    for article in get_all_articles_binance(from_dt, to_dt, inspector):\n",
    "        filename = f\"binance_{article}\"\n",
    "        if not conn.is_parsed(article):\n",
    "            try:\n",
    "                article_html = get_article_info(f\"{CONCRETE_ARTICLE_URL}{article}\", inspector)\n",
    "                article_info = parse_article_binance(article_html)\n",
    "                save_to_disk(filename, article_info)\n",
    "                conn.insert_article(article)\n",
    "            except Exception as ex:\n",
    "                print(ex)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
